{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport math\nimport random\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-10-27T14:56:58.612664Z","iopub.execute_input":"2022-10-27T14:56:58.613544Z","iopub.status.idle":"2022-10-27T14:56:58.618242Z","shell.execute_reply.started":"2022-10-27T14:56:58.613508Z","shell.execute_reply":"2022-10-27T14:56:58.617425Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class Data_Handler():\n    def __init__(self):\n        pass\n    \n    def load_data(self):\n        path = r\"../input/datasets-for-regression/Bank_Customer_Churn_Prediction/Bank_Customer_Churn_Prediction.csv\"\n    \n        df = pd.read_csv(path, header=None, delimiter=\",\")\n        df = df.drop(0, axis = 1)\n        features = df.iloc[1:, 0:10]\n        labels = df.iloc[1:, -1]\n        return (features, labels)\n\n    def one_hot_encode(self, features):\n        encoded_country = pd.get_dummies(features[2])\n        encoded_gender = pd.get_dummies(features[3])\n    \n        merged_columns = pd.concat([encoded_country, encoded_gender], axis = \"columns\")\n        features = pd.concat([features, merged_columns], axis = \"columns\")\n        features = features.drop([2, 3], axis = \"columns\")\n        return features\n    \n    def split_data(self, features, labels):\n        total_samples = features.shape[0]\n        feature_columns = features.columns.values.tolist()\n        test_split_size = int(np.ceil((20 / 100) * total_samples))\n    \n        train_x, test_x, train_y, test_y = train_test_split(features, labels, test_size = test_split_size)\n\n        train_x = train_x.reset_index(drop = True)\n        test_x = test_x.reset_index(drop = True)\n        train_y = train_y.reset_index(drop = True)\n        test_y = test_y.reset_index(drop = True)\n    \n        train_y = train_y.astype(float)\n        test_y = test_y.astype(float)\n\n        for column in feature_columns:\n            train_x[column] = train_x[column].astype(float)\n            test_x[column] = test_x[column].astype(float)\n\n        return (train_x, test_x, train_y, test_y)\n    \n    def min_max_normalization(self, df):\n        normalized_df = (df - df.min()) / (df.max() - df.min())\n        return normalized_df","metadata":{"execution":{"iopub.status.busy":"2022-10-27T14:56:58.635590Z","iopub.execute_input":"2022-10-27T14:56:58.636218Z","iopub.status.idle":"2022-10-27T14:56:58.648361Z","shell.execute_reply.started":"2022-10-27T14:56:58.636182Z","shell.execute_reply":"2022-10-27T14:56:58.647439Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def plot_data(train_costs, test_costs, epochs):\n    plt.plot(epochs, train_costs, 'g', label = \"Training Cost\")\n    plt.plot(epochs, test_costs, 'r', label = \"Testing Cost\")\n    plt.legend()\n    plt.gca().set_ylim(top = 1)\n    plt.gca().set_ylim(bottom = 0)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-27T14:56:58.650304Z","iopub.execute_input":"2022-10-27T14:56:58.651394Z","iopub.status.idle":"2022-10-27T14:56:58.667360Z","shell.execute_reply.started":"2022-10-27T14:56:58.651346Z","shell.execute_reply":"2022-10-27T14:56:58.666407Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class Neural_Network():\n    def __init__(self, epochs, lr, hidden_nodes):\n        self.epochs = epochs\n        self.lr = lr\n        self.hidden_nodes = hidden_nodes\n        \n        # Getting features and labels\n        dataset = Data_Handler()\n        self.features, self.labels = dataset.load_data()\n        print(f\"Fetures Before One Hot Encoding \\n{self.features}\\n\" + \"-\" * 150, \"\\n\")\n        print(f\"Labels \\n{self.labels}\\n\" + \"-\" * 150, \"\\n\")\n        \n        # Applying One Hot Encoding\n        self.features = dataset.one_hot_encode(self.features)\n        print(f\"Fetures After One Hot Encoding \\n{self.features}\\n\" + \"-\" * 150, \"\\n\")\n        \n        # Spilitting data into training and testing subsets\n        self.train_f, self.test_f, self.train_l, self.test_l = dataset.split_data(self.features, self.labels)\n        print(f\"Training Features \\n{self.train_f}\\n\" + \"-\" * 150, \"\\n\")\n        print(f\"Testing Features \\n{self.test_f}\\n\" + \"-\" * 150, \"\\n\")\n        print(f\"Training Labels \\n{self.train_l}\\n\" + \"-\" * 150, \"\\n\")\n        print(f\"Testing Labels \\n{self.test_l}\\n\" + \"-\" * 150, \"\\n\")\n        \n        # Normalizing features\n        self.train_f = dataset.min_max_normalization(self.train_f)\n        self.test_f = dataset.min_max_normalization(self.test_f)\n        print(f\"Training Features After Min-Max Normalization \\n{self.train_f}\\n\" + \"-\" * 150, \"\\n\")\n        print(f\"Testing Features After Min-Max Normalization \\n{self.test_f}\\n\" + \"-\" * 150, \"\\n\")\n        \n        # Initializing weights and biases for both layers\n        self.weights_layer1, self.weights_layer2, self.biases_layer1, self.biases_layer2 = self.init_parameters(13, self.hidden_nodes, 1)\n        \n    def cost_function(self, y, y_hat, total_samples):\n        y_hat.shape = (total_samples,)\n        cost = (-1 / total_samples) * np.sum(y * np.log(0.0001 + y_hat) + (1 - y) * np.log(0.0001 + 1 - y_hat))\n        return cost\n    \n    def sigmoid(self, z):\n        z = 1 / (1 + np.exp(-z))\n        return z\n\n    def relu(self, z):\n        return np.maximum(z, 0)\n    \n    def init_parameters(self, input_features, hidden_nodes, output_nodes):\n        num_weights_l1 = input_features * hidden_nodes\n        num_weights_l2 = hidden_nodes * output_nodes\n        num_biases_l1 = hidden_nodes\n        num_biases_l2 = output_nodes\n        \n        # Matrix of hidden_nodes * input\n        weights_layer1 = []\n        # Matrix of 1 * hidden_nodes\n        weights_layer2 = []\n        # Matrix of hidden_units * 1\n        biases_layer1 = []\n        # Matrix of 1 * 1\n        biases_layer2 = []\n        \n        for _ in range(hidden_nodes):\n            temp = []\n            for _ in range(input_features):\n                temp.append(round(random.uniform(-0.5, 0.5), 1))\n                \n            weights_layer1.append(np.array(temp))\n        \n        for _ in range(1):\n            temp = []\n            for _ in range(hidden_nodes):\n                temp.append(round(random.uniform(-0.5, 0.5), 1))\n            \n            weights_layer2.append(np.array(temp))\n            \n        for _ in range(hidden_nodes):\n            temp = []\n            for _ in range(1):\n                temp.append(round(random.uniform(-0.5, 0.5), 1))\n                \n            biases_layer1.append(np.array(temp))\n            \n        for _ in range(1):\n            temp = []\n            for _ in range(1):\n                temp.append(round(random.uniform(-0.5, 0.5), 1))\n                \n            biases_layer2.append(np.array(temp))\n        \n        return (np.array(weights_layer1), np.array(weights_layer2), np.array(biases_layer1), np.array(biases_layer2))    \n    \n    def forward_pass(self, input_features, weights_layer1, weights_layer2, biases_layer1, biases_layer2):\n        # Weighted sum for layer 1\n        weighted_sums_layer1 = np.dot(weights_layer1, input_features.T)\n        weighted_sums_layer1 = np.add(weighted_sums_layer1, biases_layer1)\n        \n        # Applying Relu\n        for index in range(self.hidden_nodes):\n            weighted_sums_layer1[index] = self.relu(weighted_sums_layer1[index])\n        \n        # Weighted sum for layer 2\n        weighted_sums_layer2 = np.dot(weights_layer2, weighted_sums_layer1)\n        weighted_sums_layer2 = np.add(weighted_sums_layer2, biases_layer2)\n        \n        # Applying Sigmoid\n        y_hat = self.sigmoid(weighted_sums_layer2)\n        return y_hat, weighted_sums_layer1\n    \n    def backward_pass(self, y_hat, weighted_sums_layer1):\n        # Calculate gradient of output node\n        error_O = y_hat * (1 - y_hat) * (self.train_l - y_hat)\n        error_O = np.mean(error_O)\n        \n        # Calculate gradient of hidden nodes\n        t1 = np.dot(error_O, self.weights_layer2)\n        t2 = 1 - weighted_sums_layer1\n        t3 = np.dot(weighted_sums_layer1, t2.T)\n        errors_H = np.dot(t1, t3)\n        \n        # Calculating new parameters for layer 2\n        weights_layer2 = self.weights_layer2 + np.mean(np.dot(self.lr, np.dot(error_O, weighted_sums_layer1)))\n        biases_layer2 = self.biases_layer2 + np.dot(self.lr, error_O)\n        \n        # Calculating new parameters for layer 1\n        means_of_feature_inputs = np.mean(self.train_f.T, axis = 1)\n        means_of_feature_inputs = np.array(means_of_feature_inputs.tolist())\n        means_of_feature_inputs.shape = (1, 13)\n        weights_layer1 = self.weights_layer1 + np.mean(np.dot(self.lr, np.dot(errors_H.T, means_of_feature_inputs)))\n        biases_layer1 = self.biases_layer1 + np.dot(self.lr, errors_H.T)\n        \n        return (weights_layer1, weights_layer2, biases_layer1, biases_layer2)\n    \n    def fit(self):\n        train_costs = []\n        test_costs = []\n        epochs_list = []\n        training_cost = 0\n        testing_cost = 0\n        total_train_samples = self.train_f.shape[0]\n        total_test_samples = self.test_f.shape[0]\n        \n        for epoch in tqdm(range(self.epochs + 1), desc = \"Progress\"):\n            y_hat, weighted_sums_layer1 = self.forward_pass(self.train_f, self.weights_layer1, self.weights_layer2, self.biases_layer1, self.biases_layer2)\n            y_pred, _ = self.forward_pass(self.test_f, self.weights_layer1, self.weights_layer2, self.biases_layer1, self.biases_layer2)\n            \n            training_cost = self.cost_function(self.train_l.T, y_hat, total_train_samples)\n            testing_cost = self.cost_function(self.test_l.T, y_pred, total_test_samples)\n            \n            train_costs.append(training_cost)    \n            test_costs.append(testing_cost)\n            epochs_list.append(epoch)\n            \n            self.weights_layer1, self.weights_layer2, self.biases_layer1, self.biases_layer2 = self.backward_pass(y_hat, weighted_sums_layer1)\n        \n        print(f\"Final Training Cost at Epoch [{epoch}]: {training_cost}\")\n        print(f\"Final Testing Cost at Epoch [{epoch}]: {testing_cost}\")\n        plot_data(train_costs, test_costs, epochs_list)","metadata":{"execution":{"iopub.status.busy":"2022-10-27T14:56:58.718930Z","iopub.execute_input":"2022-10-27T14:56:58.719963Z","iopub.status.idle":"2022-10-27T14:56:58.753892Z","shell.execute_reply.started":"2022-10-27T14:56:58.719910Z","shell.execute_reply":"2022-10-27T14:56:58.752702Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"epochs = int(input(\"Epochs> \"))\nhidden_nodes = int(input(\"Hidden Units> \"))\nlr = float(input(\"Learning Rate> \"))\nnetwork = Neural_Network(epochs, lr, hidden_nodes)\nnetwork.fit()","metadata":{"execution":{"iopub.status.busy":"2022-10-27T14:56:58.865151Z","iopub.execute_input":"2022-10-27T14:56:58.865544Z"},"trusted":true},"execution_count":null,"outputs":[]}]}